---
title: "Math 300 Lesson 35 Notes"
subtitle: "Regression Assumptions"
author: "YOUR NAME HERE"
date: "`r format(Sys.time(), '%B, %Y')`"
output:
  pdf_document:
    toc: yes
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(echo = TRUE)
```

## Objectives

1. Explain and test the assumptions for inference in a linear regression model.


## Reading

[Chapter 10.3](https://moderndive.com/10-inference-for-regression.html#regression-conditions)

## Lesson

Work through the learning check LC 10.1.

+ **LINE** is a mnemonic to help remember the assumptions. Linearity, Independence, Normality, and Equality of variance.

+ Residuals are the key element used to check assumptions. Note that independence is difficult to check unless there is a time element to the data collection which often happens in experiments. 

------------------------------------------------------------------------

### Libraries

```{r warning=FALSE,message=FALSE}
library(tidyverse)
library(infer)
library(moderndive)
```

## Problem

Let's briefly review the ideas from the reading before working the learning check.

### Data and model

Let's get the data and model again.

```{r}
evals_ch5 <- evals %>%
  select(ID, score, bty_avg, age)
glimpse(evals_ch5)
```

```{r}
score_model <- lm(score ~ bty_avg, data = evals_ch5)
```


```{r}
score_regression_points <- get_regression_points(score_model)
```

```{r}
head(score_regression_points)
```


### Linearity

When we have a simple linear regression model, we can check linearity using a scatterplot of the explanatory and response variables. For multiple regression, we need to plot fitted values and residuals. We will do both here for our problem.

```{r message=FALSE,warning=FALSE}
# Complete the code
# ggplot(evals_ch5, 
#        aes(x = ________, y = _______)) +
#   geom_point() +
#   labs(x = "Beauty Score", 
#        y = "Teaching Score",
#        title = "Relationship between teaching and beauty scores") +  
#   geom_smooth(method = "lm", se = FALSE, color="black") +
#   geom_smooth(method="loess", se=FALSE) +
#   theme_classic()
```

The `loess` function plots a smoother through the data, giving us a good indication as to the true linearity of the observed data. In this case, the linearity assumption is not bad, but there could be a small issue at the lowest beauty scores. 

We could also plot the residuals to assess linearity:

```{r message=FALSE,warning=FALSE}
ggplot(score_regression_points, 
       aes(x = score_hat, y = residual)) +
  geom_point() +
  labs(x = "Fitted Value", 
       y = "Residual",
       title = "Checking linearity assumption") +  
  geom_smooth(method = "lm", se = FALSE, color="black") +
  geom_smooth(method="loess", se=FALSE) +
  theme_classic()
```

We see a similar result; we prefer to use residual plots to check linearity because they allow us to generalize to multiple predictors. 

### Normality

We will build a histogram to check this assumption.

```{r}
# Complete the code
# ggplot(_______________, aes(x = _______)) +
#   geom_histogram(binwidth = 0.25, fill = "cyan", color="black") +
#   labs(x = "Residual") +
#   theme_classic()
```

There is a skew to the left.

### Equality of Variance 

For simple linear regression, we could use the original scatterplot of the data. However, we generally like to use the residuals vs fitted plot because we can extend to multiple regression. We want an equal spread around the horizontal line centered at zero. 

```{r}
# Complete the code
# ggplot(____________, aes(x = __________, y = _______)) +
#   geom_point() +
#   labs(x = "__________", y = "___________") +
#   geom_hline(yintercept = 0, col = "blue", size = 1)
```

***


## Learning Check 10.1 (Objective 1)

**(LC 10.1)** Continuing with our regression using `age` as the explanatory variable and teaching `score` as the outcome variable.

- Use the `get_regression_points()` function to get the observed values, fitted values, and residuals for all instructors. 

```{r, eval=TRUE, echo=TRUE}
# Complete the code to get the regression model and residuals

```

- Perform a residual analysis and look for any systematic patterns in the residuals. Ideally, there should be little to no pattern but comment on what you find here.

The first condition is that the relationship between the outcome variable $y$ and the explanatory variable $x$ must be **L**inear. 

```{r fig.height=3.3}
# Complete the code
set.seed(76)
# evals_ch5 %>%
#   ggplot(aes(x = ________, y = ______)) +
#   geom_point() +
#   labs(x = "_______", y = "_____________") +
#   geom_smooth(method = "lm", se = FALSE, color="black") +
#   geom_smooth(method = "loess", se=FALSE, color="blue") +
#   expand_limits(y = 10)
```  


The second condition is that the residuals must be **I**ndependent. In other words, the different observations in our data must be independent of one another. As explained in the reading, "we say there exists *dependence* between observations".

The third condition is that the residuals should follow a **N**ormal distribution.  

```{r fig.height=3.3}
# Complete the code
# ggplot(regression_points, aes(x = ______________)) +
#   geom_histogram(binwidth = 0.25, color = "black", fill = "cyan") +
#   labs(x = "__________") +
#   theme_classic()
```



The fourth and final condition is that the residuals should exhibit **E**qual variance across all values of the explanatory variable $x$. In other words, the value and spread of the residuals should not depend on the value of the explanatory variable $x$.  

```{r}
# Complete the code
# ggplot(regression_points, aes(x = _________, y = __________)) +
#   geom_point() +
#   labs(x = "___________", y = "_______________") +
#   geom_hline(yintercept = 0, col = "blue", size = 1)
```  



***

## Documenting software

-   File creation date: `r Sys.Date()`
-   `r R.version.string`
-   `tidyverse` package version: `r packageVersion("tidyverse")`
-   `moderndive` package version: `r packageVersion("moderndive")`
-   `infer` package version: `r packageVersion("infer")`

