ggplot(credit_ch6, aes(x = credit_rating, y = debt)) +
geom_point() +
labs(
x = "Credit rating", y = "Credit card debt (in $)",
title = "Debt and credit rating"
) +
geom_smooth(method = "lm", se = FALSE)
ggplot(credit_ch6, aes(x = age, y = debt)) +
geom_point() +
labs(
x = "Age (in year)", y = "Credit card debt (in $)",
title = "Debt and age"
) +
geom_smooth(method = "lm", se = FALSE)
knitr::opts_chunk$set(echo = TRUE)
alaska_flights <- flights %>%
filter(carrier == "Alaska")
library(nycflights13)
library(ggplot2)
library(dplyr)
alaska_flights <- flights %>%
filter(carrier == "Alaska")
#Plot to use for this problem.
ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) +
geom_point()
#Second Plot to use for this problem.
ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) +
geom_point(alpha = 0.2)
?flights
?airlines
flights$carrier
alaska_flights <- flights %>%
filter(carrier == "AS")
#Plot to use for this problem.
ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) +
geom_point()
#Second Plot to use for this problem.
ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) +
geom_point(alpha = 0.2)
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(moderndive)
library(skimr)
library(ISLR)
model_2_interaction <- lm(average_sat_math ~ perc_disadvan * size,
data = MA_schools)
get_regression_table(model_2_interaction)
model_2_parallel_slopes <- lm(average_sat_math ~ perc_disadvan + size,
data = MA_schools)
get_regression_table(model_2_parallel_slopes)
model_2_interaction
summary(model_2_interaction)
get_regression_summaries(model_2_interaction)
get_regression_summaries(model_2_parallel_slopes)
get_regression_summaries(score_model_interaction)
get_regression_summaries(score_model_interaction) %>%
rbind(get_regression_summaries(score_model_interaction))
get_regression_summaries(score_model_interaction) %>%
rbind(get_regression_summaries(score_model_parallel_slopes))
get_regression_summaries(score_model_interaction) %>%
rbind(get_regression_summaries(score_model_parallel_slopes)) %>%
mutate(model=c("Interaction","Parallel Slopes")) %>%
select(model,r_squared,adj_r_squared)
install.packages("palmerpenguins")
penguin_df<-
palmerpenguins::penguins %>%
na.omit()
head(penguin_df)
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
labs(x="Length", y="Depth", title="Bill Depth as a function of Bill Length") +
theme_classic()
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm)) +
geom_point() +
labs(x="Length", y="Depth", title="Bill Depth as a function of Bill Length") +
theme_classic() +
geom_smooth(method = "lm", se = FALSE)
penguin_df %>%
ggplot(aes(x=bill_length_mm, y=bill_depth_mm,
color=species)) +
geom_point() +
labs(x="Length", y="Depth", title="Bill Depth as a function of Bill Length") +
theme_classic() +
geom_smooth(method = "lm", se = FALSE)
hate_crimes <- read_csv("http://bit.ly/2ItxYg3")
library(ggplot2)
library(dplyr)
library(moderndive)
library(readr)
library(ggplot2)
library(dplyr)
library(moderndive)
library(readr)
hate_crimes <- read_csv("http://bit.ly/2ItxYg3")
glimpse(hate_crimes)
View(hate_crimes)
ggplot(data = hate_crimes, aes(y = hate_crimes, x = share_pop_metro)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(x = "Percentage of white households in poverty",
y = "Percentage of voters supporting Trump",
title = "White poverty and Trump support in the US")
hate_crimes
summary(hate_crimes)
summary(hate_crimes)
hate_crimes_ps <- hate_crimes %>%
select(state, hate_crimes, share_pop_metro, urbanization) %>%
drop_na()
hate_crimes_ps <- hate_crimes %>%
select(state, hate_crimes, share_pop_metro, urbanization) %>%
na.omit()
summary(hate_crimes_ps)
ggplot(data = hate_crimes_ps, aes(y = hate_crimes, x = share_pop_metro)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(x = "Percentage of white households in poverty",
y = "Percentage of voters supporting Trump",
title = "White poverty and Trump support in the US")
ggplot(data = hate_crimes_ps, aes(y = hate_crimes, x = share_pop_metro)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(x = "Percentage of white households in poverty",
y = "Percentage of voters supporting Trump",
title = "White poverty and Trump support in the US")
ggplot(data = hate_crimes_ps, aes(y = hate_crimes, x = share_pop_metro)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(x = "Percentage of state population living in metro area",
y = "Number of Hate Crimes per 100K people",
title = "Urbanization and Hate Crimes in the US")
urban_mod <- lm(hate_crimes ~ share_pop_metro, data = hate_crimes_ps)
get_regression_table(urban_mod)
y_hat = 0.117 + 0.242 * .82
y_hat
head(hate_crimes_ps)
summary(hate_crimes_ps)
y_hat = 0.117 + 0.242 * .82
y_hat
summary(hate_crimes_ps)
library(ggplot2)
library(dplyr)
library(moderndive)
library(readr)
ggplot(data = hate_crimes, aes(x = trump_support, y = hate_crimes)) +
geom_boxplot() +
labs(x = "Voter Support of Trump",
y = "Number of Hate Crimes per 100K people",
title = "Hate Crimes in relation to Trump Support")
ggplot(data = hate_crimes, aes(x = urbanization, y = hate_crimes)) +
geom_boxplot() +
labs(x = "Urbanization",
y = "Number of Hate Crimes per 100K people",
title = "Hate Crimes in relation to Urbanization") +
theme_classic()
ggplot(data = hate_crimes, aes(x = urbanization, y = hate_crimes_ps)) +
geom_boxplot() +
labs(x = "Urbanization",
y = "Number of Hate Crimes per 100K people",
title = "Hate Crimes in relation to Urbanization") +
theme_classic()
View(hate_crimes_ps)
ggplot(data = hate_crimes_ps, aes(x = urbanization, y = hate_crimes)) +
geom_boxplot() +
labs(x = "Urbanization",
y = "Number of Hate Crimes per 100K people",
title = "Hate Crimes in relation to Urbanization") +
theme_classic()
hate_mod <- lm(hate_crimes ~ urbanization, data = hate_crimes_ps)
get_regression_table(hate_mod)
y_hat = 0.352 - 0.098 * 1
y_hat
library(ggplot2)
library(dplyr)
library(moderndive)
library(readr)
ggplot(data = hate_crimes, aes(x = urbanization, y = income)) +
geom_boxplot(fill = c("sienna", "darkgreen")) +
labs(x = "Level of urbanization in a state", y = "Median household income in USD $",
title = "Household income across an urbanization gradient in 2016") +
theme_classic()
library(ggplot2)
library(dplyr)
library(moderndive)
library(readr)
summary(hate_crimes)
hate_crimes_ps4 <- hate_crimes %>%
select(state, hs, income, urbanization) %>%
na.omit()
hate_crimes_ps4 <- hate_crimes %>%
select(state, hs, income, urbanization) %>%
na.omit()
ggplot(data = hate_crimes_ps4, aes(x = urbanization, y = income)) +
geom_boxplot(fill = c("sienna", "darkgreen")) +
labs(x = "Level of urbanization in a state", y = "Median household income in USD $",
title = "Household income across an urbanization gradient in 2016") +
theme_classic()
ggplot(data = hate_crimes_ps4, aes(y = income, x = hs, color = urbanization)) +
geom_point()+
geom_parallel_slopes(se = FALSE) +
labs(x = "Percent of adults with high-school degree",
y = "Median household income in USD $",
title = "Education and income in states with differing levels of urbanization"
) +
theme_classic()
ggplot(data = hate_crimes_ps4, aes(y = income, x = hs, color = urbanization)) +
geom_point()+
geom_smooth(method = "lm", se = FALSE) +
labs(
x = "Percent of adults with high-school degree",
y = "Median household income in USD $",
title = "Education and income in states with differing levels of urbanization") +
theme
ggplot(data = hate_crimes_ps4, aes(y = income, x = hs, color = urbanization)) +
geom_point()+
geom_smooth(method = "lm", se = FALSE) +
labs(
x = "Percent of adults with high-school degree",
y = "Median household income in USD $",
title = "Education and income in states with differing levels of urbanization") +
theme_classic()
income_parallel_model <- lm(income ~ hs + urbanization, data = hate_crimes)
income_interaction_model <- lm(income ~ hs * urbanization, data = hate_crimes)
get_regression_summaries(income_parallel_model)
get_regression_summaries(income_interaction_model)
ggplot(data = hate_crimes_ps4, aes(y = income, x = hs, color = urbanization)) +
geom_point()+
geom_smooth(method = "lm", se = FALSE) +
labs(
x = "Percent of adults with high-school degree",
y = "Median household income in USD $",
title = "Education and income in states with differing levels of urbanization") +
theme_classic()
get_regression_table(income_parallel_model)
get_regression_table(income_interaction_model)
income_parallel_model <- lm(income ~ hs + urbanization, data = hate_crimes_ps4)
income_interaction_model <- lm(income ~ hs * urbanization, data = hate_crimes_ps4)
get_regression_summaries(income_parallel_model)
get_regression_summaries(income_interaction_model)
get_regression_table(income_parallel_model)
get_regression_table(income_interaction_model)
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(moderndive)
library(tidyverse)
library(moderndive)
head(tactile_prop_red)
ggplot(tactile_prop_red, aes(x = prop_red)) +
geom_histogram(binwidth = 0.05, boundary = 0.4, color = "white") +
labs(x = "Proportion of 50 balls that were red",
title = "Distribution of 33 proportions red") +
theme_classic()
# Segment 2: sample size = 50 ------------------------------
# 2.a) Virtually use shovel 1000 times
set.seed(107)
virtual_samples_50 <- bowl %>%
rep_sample_n(size = 50, reps = 1000)
# Compute resulting 1000 replicates of proportion red
virtual_prop_red_50 <- virtual_samples_50 %>%
group_by(replicate) %>%
summarize(red = sum(color == "red")) %>%
mutate(prop_red = red / 50)
#Plot distribution via a histogram
ggplot(virtual_prop_red_50, aes(x = prop_red)) +
geom_histogram(binwidth = 0.05, boundary = 0.4, color = "white") +
labs(x = "Proportion of 50 balls that were red", title = "50")  +
theme_classic()
summary(virtual_prop_red_50)
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(echo = TRUE)
set.seed(76)
comparing_n_table <- virtual_prop %>%
group_by(n) %>%
summarize(sd = sd(prop_red)) %>%
mutate(
n = str_c("n = ")
) %>%
rename(`Sample size` = n, `Standard error of $\\widehat{p}$` = sd) %>%
sample_frac(1)
virtual_prop_red_50
bowl %>%
rep_sample_n(size = 50, reps = 10)
bowl %>%
rep_sample_n(size = 50, reps = 10) %>%
group_by(replicate) %>%
summarize(red = sum(color == "red")) %>%
mutate(prop_red = red / 50, n=50)
set.seed(76)
vp50<-bowl %>%
rep_sample_n(size = 50, reps = 1000) %>%
group_by(replicate) %>%
summarize(red = sum(color == "red")) %>%
mutate(prop_red = red / 50, n=50)
vp100<-bowl %>%
rep_sample_n(size = 100, reps = 1000) %>%
group_by(replicate) %>%
summarize(red = sum(color == "red")) %>%
mutate(prop_red = red / 100, n=100)
vp50<-bowl %>%
rep_sample_n(size = 25, reps = 1000) %>%
group_by(replicate) %>%
summarize(red = sum(color == "red")) %>%
mutate(prop_red = red / 25, n=25)
virtual_prop<-rbind(vp25,vp50,vp100)
set.seed(76)
vp50<-bowl %>%
rep_sample_n(size = 50, reps = 1000) %>%
group_by(replicate) %>%
summarize(red = sum(color == "red")) %>%
mutate(prop_red = red / 50, n=50)
vp100<-bowl %>%
rep_sample_n(size = 100, reps = 1000) %>%
group_by(replicate) %>%
summarize(red = sum(color == "red")) %>%
mutate(prop_red = red / 100, n=100)
vp25<-bowl %>%
rep_sample_n(size = 25, reps = 1000) %>%
group_by(replicate) %>%
summarize(red = sum(color == "red")) %>%
mutate(prop_red = red / 25, n=25)
virtual_prop<-rbind(vp25,vp50,vp100)
comparing_n_table <- virtual_prop %>%
group_by(n) %>%
summarize(sd = sd(prop_red)) %>%
mutate(
n = str_c("n = ")
) %>%
rename(`Sample size` = n, `Standard error of $\\widehat{p}$` = sd) %>%
sample_frac(1)
comparing_n_table %>%
kable(
digits = 3,
caption = "Three standard errors of the sample proportion based on n = 25, 50, 100",
booktabs = TRUE,
escape = FALSE,
linesep = ""
) %>%
kable_styling(
font_size = ifelse(is_latex_output(), 10, 16),
latex_options = c("hold_position")
)
xlim <- c(-6, 6)
p1 <- ggplot(data = data.frame(x = xlim), aes(x)) +
stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = .5)) +
ylab("") +
# scale_y_continuous(breaks = NULL)+
geom_vline(xintercept = 0, linetype = "dotted") +
coord_cartesian(xlim = xlim, ylim = c(0, 0.8)) +
ggtitle("High precision, high accuracy")
p2 <- ggplot(data = data.frame(x = xlim), aes(x)) +
stat_function(fun = dnorm, n = 101, args = list(mean = 2.5, sd = .5)) +
ylab("") +
# scale_y_continuous(breaks = NULL)+
geom_vline(xintercept = 0, linetype = "dotted") +
coord_cartesian(xlim = xlim, ylim = c(0, 0.8)) +
ggtitle("High precision, low accuracy")
p3 <- ggplot(data = data.frame(x = xlim), aes(x)) +
stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1.5)) +
ylab("") +
# scale_y_continuous(breaks = NULL)+
geom_vline(xintercept = 0, linetype = "dotted") +
coord_cartesian(xlim = xlim, ylim = c(0, 0.8)) +
ggtitle("Low precision, high accuracy")
p4 <- ggplot(data = data.frame(x = xlim), aes(x)) +
stat_function(fun = dnorm, n = 101, args = list(mean = 2.5, sd = 1.5)) +
ylab("") +
# scale_y_continuous(breaks = NULL)+
geom_vline(xintercept = 0, linetype = "dotted") +
coord_cartesian(xlim = xlim, ylim = c(0, 0.8)) +
ggtitle("Low precision, low accuracy")
library(patchwork)
(p1 | p2) / (p3 | p4)
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(echo = TRUE)
set.seed(316)
runif(4,5,6)
set.seed(316)
round(runif(4,5,6),2)
set.seed(316)
time<-round(runif(4,5,6),2)
set.seed(316)
(time<-round(runif(4,5,6),2))
mean(time)
?replicate
replicate(2,mean(round(runif(4,5,6),2)))
set.seed(316)
(time<-round(runif(4,5,6),2))
replicate(2,mean(round(runif(4,5,6),2)))
set.seed(316)
(time<-round(runif(4,5,6),2))
# Repeat this 1000 times
rep_data<-replicate(1000,mean(round(runif(4,5,6),2)))
summary(rep_data)
set.seed(316)
(time<-round(runif(4,5,6),2))
# Repeat this 1000 times
rep_data<-tibble(ave=replicate(1000,mean(round(runif(4,5,6),2))))
library(tidyverse)
library(tidyverse)
set.seed(316)
(time<-round(runif(4,5,6),2))
# Repeat this 1000 times
rep_data<-tibble(ave=replicate(1000,mean(round(runif(4,5,6),2))))
summary(rep_data)
ggplot(rep_data,aes(x=ave)) +
geom_histogram()
ggplot(rep_data,aes(x=ave)) +
geom_histogram()
xlim(5.1,5.9)
ggplot(rep_data,aes(x=ave)) +
geom_histogram()
ggplot(rep_data,aes(x=ave)) +
geom_histogram() +
xlim(5.1,5.9)
ggplot(rep_data,aes(x=ave)) +
geom_histogram() +
xlim(5.1,5.9)
# Repeat this 1000 times
rep_data2<-tibble(ave=replicate(1000,mean(round(runif(10,5,6),2))))
ggplot(rep_data,aes(x=ave)) +
geom_histogram() +
xlim(5.1,5.9) +
theme_classic()
ggplot(rep_data2,aes(x=ave)) +
geom_histogram() +
xlim(5.1,5.9) +
theme_classic()
pnorm(3,mu=6,sd=3)
?prnorm
?pnorm
pnorm(3,mean=6,sd=3)
pnorm(12,mean=6,sd=3,lower.tail = FALSE)
pnorm(12,mean=6,sd=3)-pnorm(0,mean=6,sd=3)
qnorm(.025,mean=6,sd=3)
?qnorm
qnorm(.975,mean=6,sd=3)
qnorm(1,mean=6,sd=3)
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(moderndive)
library(infer)
pennies
pennies_sample
head(pennies_sample)
pennies_sample %>%
summarize(mean_year = mean(year))
ggplot(pennies_sample, aes(x = year)) +
geom_histogram(binwidth = 10, color = "white") +
theme_classic()
head(pennies_resamples)
pennies_resamples %>%
select(replicate==1) %>%
ggplot(aes(x = year)) +
geom_histogram(binwidth = 10, color = "white") +
theme_classic()
pennies_resamples %>%
filter(replicate==1) %>%
ggplot(aes(x = year)) +
geom_histogram(binwidth = 10, color = "white") +
theme_classic()
resampled_means <- pennies_resamples %>%
group_by(name) %>%
summarize(mean_year = mean(year)) %>%
ggplot(aes(x = mean_year)) +
geom_histogram(binwidth = 10, color = "white") +
theme_classic()
resampled_means <- pennies_resamples %>%
group_by(name) %>%
summarize(mean_year = mean(year)) %>%
ggplot(aes(x = mean_year)) +
geom_histogram(binwidth = 10, color = "white") +
theme_classic()
resampled_means <- pennies_resamples %>%
group_by(name) %>%
summarize(mean_year = mean(year)) %>%
ggplot(aes(x = mean_year)) +
geom_histogram(binwidth = 1, color = "white", boundary = 1990) +
theme_classic()
pennies_resamples %>%
group_by(name) %>%
summarize(mean_year = mean(year)) %>%
ggplot(aes(x = mean_year)) +
geom_histogram(binwidth = 1, color = "white", boundary = 1990) +
theme_classic()
head(pennies_sample)
#Set a seed for reproducibility of results.
set.seed(84337)
# Samples of size 50 repeated 1000 times
virtual_resampled_means <- pennies_sample %>%
rep_sample_n(size = 50, replace = TRUE, reps = 1000) %>%
group_by(replicate) %>%
summarize(mean_year = mean(year))
head(virtual_resampled_means)
# An estiamte of the population mean would be the mean of the bootstrap distribution
virtual_resampled_means %>%
summarize(mean_of_means = mean(mean_year))
# The bootstrap distribution
ggplot(virtual_resampled_means, aes(x = mean_year)) +
geom_histogram(binwidth = 1, color = "white", boundary = 1990) +
labs(x = "sample mean") +
theme_classic()
virtual_resampled_means %>%
summarize(mean_of_means = mean(mean_year))
